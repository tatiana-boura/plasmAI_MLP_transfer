Ar:
  lr: 0.0009620551638036177
  batch_size: 128
  weight_decay: 2.0550345637540963e-05

O2:
  lr: 0.0029389095728573683
  batch_size: 8 #16 for baseline, 8 for tuned
  weight_decay: 1.737720320769069e-06

nn_arch:
  neurons_per_layer: 10
  layers: 3

training:
  epochs: 700
  patience: 20
